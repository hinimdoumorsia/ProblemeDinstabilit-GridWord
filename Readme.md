# GridWorld DQN â€” Apprentissage par Renforcement Profond

Ce projet illustre la diffÃ©rence entre **trois stratÃ©gies dâ€™apprentissage DQN (Deep Q-Network)** appliquÃ©es Ã  un environnement simple **GridWorld** :  

- **Agent simple (DQN sans replay memory)** : mise Ã  jour immÃ©diate Ã  chaque Ã©tape, pas de mÃ©moire dâ€™expÃ©rience.  
- **Mise Ã  jour immÃ©diate (DQN avec Experience Replay)** : le rÃ©seau est mis Ã  jour Ã  chaque Ã©tape mais Ã  partir dâ€™un mini-batch extrait de la mÃ©moire.  
- **Mise Ã  jour pÃ©riodique (DQN avec Experience Replay et Target Network)** : le rÃ©seau est mis Ã  jour aprÃ¨s un certain nombre dâ€™Ã©tapes, avec un rÃ©seau cible pour plus de stabilitÃ©.

---

## ğŸš€ Objectif du projet

Ce projet vise Ã  **comprendre les problÃ¨mes liÃ©s Ã  lâ€™apprentissage par renforcement profond (RL)** : instabilitÃ©, divergence et oscillations lors de lâ€™entraÃ®nement des agents DQN.  
Nous implÃ©mentons diffÃ©rentes stratÃ©gies pour **mettre en Ã©vidence ces problÃ¨mes et la solution proposÃ©e par DeepMind**.

---

## âš™ï¸ Principe du Bootstrapping

Le **bootstrapping** consiste Ã  **mettre Ã  jour une estimation Ã  partir dâ€™une autre estimation** plutÃ´t que de sâ€™appuyer uniquement sur la rÃ©compense finale.  
Formule gÃ©nÃ©rale du Q-learning :

$$
Q(s, a) \leftarrow Q(s, a) + \alpha \Big[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \Big]
$$

- $Q(s, a)$ : valeur actuelle de lâ€™action $a$ dans lâ€™Ã©tat $s$  
- $r$ : rÃ©compense immÃ©diate  
- $\gamma$ : facteur de discount  
- $s'$ : nouvel Ã©tat aprÃ¨s avoir pris lâ€™action  
- $\max_{a'} Q(s', a')$ : estimation bootstrapÃ©e de la valeur future  

Le terme $r + \gamma \max_{a'} Q(s', a')$ est appelÃ© **cible bootstrapÃ©e**.

---

## ğŸ§  Agents et Ã©tapes mathÃ©matiques

### 1ï¸âƒ£ Agent simple (sans Replay Memory)

- **Principe** : mise Ã  jour immÃ©diate Ã  chaque Ã©tape, pas de mini-batch.  
- **Formule mathÃ©matique** :

$$
y = 
\begin{cases}
r & \text{si Ã©tat terminal} \\
r + \gamma \max_{a'} Q(s', a'; \theta) & \text{sinon}
\end{cases}
$$

- **Loss** :  

$$
L(\theta) = \big( Q(s, a; \theta) - y \big)^2
$$

- **Backpropagation** : mise Ã  jour directe des poids du rÃ©seau aprÃ¨s chaque transition.

- **Exploration** : politique $\epsilon$-greedy avec dÃ©croissance progressive de $\epsilon$.

> Cet agent illustre directement **le problÃ¨me des cibles mouvantes**, car il met Ã  jour le rÃ©seau avec sa propre prÃ©diction sans stabilisation.

---

### 2ï¸âƒ£ Agent avec mise Ã  jour immÃ©diate + Replay Memory

- **Principe** : mise Ã  jour immÃ©diate, mais sur un mini-batch alÃ©atoire issu de la mÃ©moire dâ€™expÃ©rience.  
- **Formule mathÃ©matique** :

$$
y = r + \gamma \max_{a'} Q(s', a'; \theta)
$$

- **Loss** :  

$$
L(\theta) = \frac{1}{N} \sum_{i=1}^{N} \big( Q(s_i, a_i; \theta) - y_i \big)^2
$$

- **Avantage** : briser les corrÃ©lations temporelles entre transitions pour stabiliser lâ€™apprentissage.

---

### 3ï¸âƒ£ Agent avec mise Ã  jour pÃ©riodique + Target Network

- **Principe** : mise Ã  jour tous les $N$ steps, avec un **rÃ©seau cible** $Q_{\text{target}}$ pour calculer les cibles.  
- **Formule mathÃ©matique** :

$$
y = r + \gamma \max_{a'} Q_{\text{target}}(s', a'; \theta^-)
$$

- **Loss** :  

$$
L(\theta) = \frac{1}{N} \sum_{i=1}^{N} \big( Q_{\text{policy}}(s_i, a_i; \theta) - y_i \big)^2
$$

- **Mise Ã  jour du rÃ©seau cible** : tous les $M$ steps, les poids $\theta^-$ sont remplacÃ©s par ceux du rÃ©seau principal $\theta$.

> Cette approche rÃ©duit fortement lâ€™instabilitÃ© et reprÃ©sente **la solution DeepMind pour stabiliser le DQN**.

---

## âš ï¸ ProblÃ¨me dâ€™instabilitÃ© dÃ©tectÃ© par DeepMind

- **CorrÃ©lation forte entre les transitions** : les Ã©tats successifs sont dÃ©pendants, ce qui fausse lâ€™apprentissage.  
- **Cibles mouvantes** : le rÃ©seau apprend sur des cibles qui Ã©voluent en permanence.  
- **Propagation dâ€™erreurs** : une petite erreur dans la prÃ©diction dâ€™un Ã©tat futur peut se propager et amplifier lâ€™erreur.

---

## ğŸ’¡ Solutions proposÃ©es

| Solution | Description |
|----------|-------------|
| **Experience Replay** | Stocker les expÃ©riences et Ã©chantillonner un mini-batch alÃ©atoire pour casser les corrÃ©lations. |
| **Target Network** | RÃ©seau cible fixe temporairement pour fournir des cibles stables. |

---

## ğŸ§© Architecture du projet

ğŸ“¦ GridWorld-DQN  
â”œâ”€â”€ `gridworld_env.py`        # Environnement GridWorld (agent, but, actions)  
â”œâ”€â”€ `models.py`               # RÃ©seau DQN et Replay Memory  
â”œâ”€â”€ `agent_simple.py`         # Agent DQN simple (sans Replay Memory)  
â”œâ”€â”€ `agent_immediate.py`      # Agent DQN avec mise Ã  jour immÃ©diate + Replay Memory  
â”œâ”€â”€ `agent_periodic.py`       # Agent DQN avec mise Ã  jour pÃ©riodique + Target Network  
â”œâ”€â”€ `train_immediate.py`      # Script dâ€™entraÃ®nement (agent immÃ©diat)  
â”œâ”€â”€ `train_periodic.py`       # Script dâ€™entraÃ®nement (agent pÃ©riodique)  
â”œâ”€â”€ `train_simple.py`         # Script dâ€™entraÃ®nement (agent simple)  
â”œâ”€â”€ `test_agents.py`          # Tests et comparaison des agents entraÃ®nÃ©s  
â””â”€â”€ `requirements.txt`        # DÃ©pendances Python

---

## ğŸ“Œ Conclusion

Ce projet permet de **comparer les performances et la stabilitÃ© de diffÃ©rents agents DQN** :  

- Lâ€™**agent simple** montre lâ€™instabilitÃ© maximale.  
- Lâ€™**agent immÃ©diat avec Replay Memory** amÃ©liore lÃ©gÃ¨rement la stabilitÃ©.  
- Lâ€™**agent pÃ©riodique avec Target Network** montre la meilleure stabilitÃ©, illustrant **la solution de DeepMind** pour le Deep Reinforcement Learning.  

Ainsi, le projet est une **dÃ©monstration pÃ©dagogique complÃ¨te** des **problÃ¨mes dâ€™apprentissage profond avec bootstrapping** et des **solutions pour stabiliser les DQN**.
