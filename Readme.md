# GridWorld DQN â€” Apprentissage par Renforcement Profond

Ce projet illustre la diffÃ©rence entre **deux stratÃ©gies dâ€™apprentissage DQN (Deep Q-Network)** appliquÃ©es Ã  un environnement simple **GridWorld** :  
- **Mise Ã  jour immÃ©diate** : le rÃ©seau est mis Ã  jour Ã  chaque Ã©tape.  
- **Mise Ã  jour pÃ©riodique** : le rÃ©seau est mis Ã  jour aprÃ¨s un certain nombre dâ€™Ã©tapes, avec un **rÃ©seau cible** pour plus de stabilitÃ©.

---

## ğŸš€ Objectif du projet

Dans cette partie du projet, nous allons essayer dâ€™implÃ©menter **des approches diffÃ©rentes** en utilisant un agent dans un **GridWorld** avec un **but qui se dÃ©place**.  
Lâ€™objectif est de **mettre en exergue le problÃ¨me dâ€™instabilitÃ©** rencontrÃ© dans le Deep Reinforcement Learning.  

En premier temps, nous implÃ©mentons le **DQN classique de DeepMind avec mise Ã  jour immÃ©diate**, puis nous comparons avec une **approche utilisant une mise Ã  jour pÃ©riodique**, qui reprÃ©sente la **solution proposÃ©e par DeepMind pour rÃ©soudre le problÃ¨me de divergence**.

---

## âš™ï¸ Principe du Bootstrapping

Le **bootstrapping** consiste Ã  **mettre Ã  jour une estimation Ã  partir dâ€™une autre estimation** plutÃ´t que de sâ€™appuyer uniquement sur une rÃ©compense finale.  
Autrement dit, lâ€™agent apprend non seulement Ã  partir des rÃ©compenses immÃ©diates, mais aussi Ã  partir des **valeurs estimÃ©es des Ã©tats futurs**.

La mise Ã  jour Q-learning classique repose sur :

$$
Q(s, a) \leftarrow Q(s, a) + \alpha \Big[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \Big]
$$

- $Q(s, a)$ : valeur actuelle de lâ€™action $a$ dans lâ€™Ã©tat $s$  
- $r$ : rÃ©compense immÃ©diate  
- $\gamma$ : facteur de discount  
- $s'$ : nouvel Ã©tat aprÃ¨s avoir pris lâ€™action  
- $\max_{a'} Q(s', a')$ : estimation bootstrapÃ©e de la valeur future  

Le terme $r + \gamma \max_{a'} Q(s', a')$ est appelÃ© **cible bootstrapÃ©e**.

---

## âš ï¸ ProblÃ¨me dâ€™instabilitÃ© dÃ©tectÃ© par DeepMind

DeepMind a constatÃ© que lâ€™utilisation du bootstrapping dans un rÃ©seau de neurones profond crÃ©e une **instabilitÃ©** pour plusieurs raisons :

1. **CorrÃ©lation forte entre les Ã©chantillons successifs** : les transitions dâ€™expÃ©rience $(s, a, r, s')$ ne sont pas indÃ©pendantes.
2. **Cibles mouvantes** : comme $Q(s', a')$ est lui-mÃªme produit par le mÃªme rÃ©seau en cours dâ€™apprentissage, la cible change constamment.
3. **Propagation dâ€™erreurs** : une petite erreur dans la prÃ©diction de $Q(s', a')$ peut se propager et sâ€™amplifier lors des mises Ã  jour successives.

---

## ğŸ’¡ Solutions proposÃ©es par DeepMind

Pour stabiliser le bootstrapping dans DQN, DeepMind a introduit deux mÃ©canismes majeurs :

### 1. **Experience Replay**
Les expÃ©riences sont stockÃ©es dans une mÃ©moire $D = \{(s, a, r, s')\}$.  
Pendant lâ€™entraÃ®nement, on Ã©chantillonne **alÃ©atoirement** un mini-batch de transitions depuis cette mÃ©moire pour briser les corrÃ©lations temporelles.

### 2. **Target Network**
On utilise un **second rÃ©seau $Q_{\text{target}}$**, copie pÃ©riodique du rÃ©seau principal $Q_{\text{online}}$.  
Ce rÃ©seau fournit des cibles stables pour le bootstrapping :

$$
y = r + \gamma \max_{a'} Q_{\text{target}}(s', a'; \theta^-)
$$

Les poids $\theta^-$ du rÃ©seau cible ne sont mis Ã  jour quâ€™occasionnellement, rÃ©duisant les oscillations et amÃ©liorant la stabilitÃ©.

---

## ğŸ§  En rÃ©sumÃ©

| Concept | Description |
|----------|--------------|
| **Bootstrapping** | Utiliser les valeurs estimÃ©es dâ€™Ã©tats futurs pour mettre Ã  jour les valeurs actuelles. |
| **ProblÃ¨me** | Cibles mouvantes et corrÃ©lations entre les Ã©chantillons â†’ instabilitÃ©. |
| **Solution DeepMind** | Experience Replay + Target Network. |

GrÃ¢ce Ã  ces techniques, **DQN** a pu combiner **le bootstrapping**, **lâ€™apprentissage profond** et **lâ€™exploration** pour battre les humains dans plusieurs jeux Atari.

---

## ğŸ§© Architecture du projet

# Architecture du projet GridWorld DQN

ğŸ“¦ GridWorld-DQN  
â”œâ”€â”€ gridworld_env.py        # Environnement GridWorld (agent, but, actions)  
â”œâ”€â”€ models.py               # RÃ©seau DQN et mÃ©moire dâ€™expÃ©rience (ReplayBuffer)  
â”œâ”€â”€ agent_immediate.py      # Agent DQN avec mise Ã  jour immÃ©diate  
â”œâ”€â”€ agent_periodic.py       # Agent DQN avec mise Ã  jour pÃ©riodique  
â”œâ”€â”€ train_immediate.py      # Script dâ€™entraÃ®nement (agent immÃ©diat)  
â”œâ”€â”€ train_periodic.py       # Script dâ€™entraÃ®nement (agent pÃ©riodique)  
â”œâ”€â”€ test_agents.py          # Tests et comparaison des agents entraÃ®nÃ©s  
â””â”€â”€ requirements.txt        # DÃ©pendances Python
