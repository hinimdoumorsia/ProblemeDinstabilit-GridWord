# RÃ©sultats des Tests des Agents RL dans GridWorld

## Chargement des Agents
- âœ… Agent immÃ©diat chargÃ© avec succÃ¨s
- âœ… Agent pÃ©riodique chargÃ© avec succÃ¨s
- âœ… Agent simple (sans replay memory) chargÃ© avec succÃ¨s

---

## 1ï¸âƒ£ Test de l'Agent : Mise Ã  Jour ImmÃ©diate

### Ã‰pisodes 1 Ã  3
- **DÃ©part** : Agent `[0, 0]`, But `[4, 4]`
- **Trajectoire** : `[0,0] â†’ [0,1] â†’ [0,2] â†’ [0,3] â†’ [1,3] â†’ [2,3] â†’ [3,3] â†’ [3,4] â†’ [4,4]`
- **RÃ©compenses** : -0.1 par Ã©tape sauf derniÃ¨re Ã©tape (+10)
- **Steps** : 8
- **Score final** : 9.30
- ğŸ‰ **But atteint !**

> Observation : Lâ€™agent immÃ©diat atteint le but de maniÃ¨re optimale en 8 Ã©tapes Ã  chaque Ã©pisode.

---

## 2ï¸âƒ£ Test de l'Agent : Mise Ã  Jour PÃ©riodique (Replay Memory)

### Ã‰pisodes 1 Ã  3
- **DÃ©part** : Agent `[0, 0]`, But `[4, 4]`
- **Trajectoire** : `[0,0] â†’ [1,0] â†’ [2,0] â†’ [3,0] â†’ [4,0] â†’ [4,1] â†’ [4,2] â†’ [4,3] â†’ [4,4]`
- **RÃ©compenses** : -0.1 par Ã©tape sauf derniÃ¨re Ã©tape (+10)
- **Steps** : 8
- **Score final** : 9.30
- ğŸ‰ **But atteint !**

> Observation : Lâ€™agent pÃ©riodique utilise une **replay memory** pour consolider ses expÃ©riences et stabiliser lâ€™apprentissage.

---

## 3ï¸âƒ£ Test de l'Agent : Simple (Sans Replay Memory)

### Ã‰pisodes 1 Ã  5
- **DÃ©part** : Agent `[0, 0]`, But `[4, 4]`
- **Trajectoire** : `[0,0] â†’ [1,0] â†’ [2,0] â†’ [3,0] â†’ [4,0] â†’ [4,1] â†’ [4,2] â†’ [4,3] â†’ [4,4]`
- **RÃ©compenses** : -0.1 par Ã©tape sauf derniÃ¨re Ã©tape (+10)
- **Steps** : 8
- **Score final** : 9.30
- ğŸ‰ **But atteint !**

> Observation : Lâ€™agent simple **sans replay memory** met Ã  jour ses Q-values Ã  chaque Ã©tape. MalgrÃ© lâ€™absence de mÃ©moire, il atteint le but efficacement dans ce GridWorld simple.  

---

## 4ï¸âƒ£ Performance Quantitative

| Agent                   | Score moyen | SuccÃ¨s | Steps moyens | Meilleur score | Pire score |
|-------------------------|------------|--------|--------------|----------------|------------|
| ImmÃ©diat                | 9.30 Â± 0.00 | 100%   | 8.0          | 9.30           | 9.30       |
| PÃ©riodique (Replay)     | 9.30 Â± 0.00 | 100%   | 8.0          | 9.30           | 9.30       |
| Simple (Sans Replay)    | 9.30 Â± 0.00 | 100%   | 8.0          | 9.30           | 9.30       |

> Tous les agents atteignent le but de maniÃ¨re optimale dans ce GridWorld 5x5.

---

## 5ï¸âƒ£ Comparaison Finale
- ğŸ”µ **ImmÃ©diat** : Score 9.30, SuccÃ¨s 100%
- ğŸ”´ **PÃ©riodique avec replay memory** : Score 9.30, SuccÃ¨s 100%
- ğŸŸ¢ **Simple sans replay memory** : Score 9.30, SuccÃ¨s 100%
- âš–ï¸ **Conclusion** : Les performances sont similaires ici, mais la **replay memory** reste un avantage pour des environnements plus complexes.

---

## 6ï¸âƒ£ Visualisations

### 1ï¸âƒ£ Courbe des scores par Ã©pisode - Agent immÃ©diat
![Courbe ImmÃ©diat](images/immediate_training.png)

### 2ï¸âƒ£ Courbe des scores par Ã©pisode - Agent pÃ©riodique
![Courbe PÃ©riodique](images/periodic_training.png)

### 3ï¸âƒ£ Courbe des scores par Ã©pisode - Agent simple sans replay memory
![Courbe Simple](images/simple_agent_training.png)

> Les visualisations permettent de comparer la stabilitÃ© et la rÃ©pÃ©tabilitÃ© des trois approches.

---

### ğŸ“ RÃ©sumÃ©
- Lâ€™**agent immÃ©diat** privilÃ©gie un chemin horizontal puis vertical.  
- Lâ€™**agent pÃ©riodique avec replay memory** consolide ses expÃ©riences pour stabiliser lâ€™apprentissage.  
- Lâ€™**agent simple** met Ã  jour ses Q-values directement, sans mÃ©moire, et rÃ©ussit efficacement dans ce GridWorld simple.  
- Les visualisations confirment la constance des scores et le succÃ¨s de chaque Ã©pisode.
---
